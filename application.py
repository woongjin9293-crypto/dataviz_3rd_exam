import streamlit as st
import pandas as pd
import numpy as np
import re
from datetime import datetime, date

import matplotlib.pyplot as plt
import seaborn as sns
import altair as alt
import plotly.express as px

from wordcloud import WordCloud, STOPWORDS
import networkx as nx
from itertools import combinations
from collections import Counter

from konlpy.tag import Okt
import koreanize_matplotlib
from matplotlib import font_manager


st.set_page_config(
    page_title="K-POP Demon Hunters íŒ¬ë¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ™ˆ",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.title("2025 Netflix 'K-POP Demon Hunters' íŒ¬ë¤ í˜•ì„± ìš”ì¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.write("í•™ë²ˆ/ì´ë¦„: C221084 ë°±ì§„ì›…")
st.caption("ë°ì´í„°: ë„¤ì´ë²„ ê²€ìƒ‰ API (News/Blog), í‚¤ì›Œë“œ-ê°ì„±-ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ë¶„ì„")
st.divider()


NEWS_PATH = "kpop_demon_hunters_news.csv"
BLOG_PATH = "kpop_demon_hunters_blog.csv"


@st.cache_data
def load_data(path: str, source: str) -> pd.DataFrame:
    df = pd.read_csv(path)

    # ë‚ ì§œê°€ ê¹¨ì§„ í–‰ì´ ìˆìœ¼ë©´ ì´í›„ í•„í„°ì—ì„œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆì–´ì„œ ë¨¼ì € ì •ë¦¬í•¨.
    df["pubDate"] = pd.to_datetime(df["pubDate"], errors="coerce")
    df = df.dropna(subset=["pubDate"]).copy()

    # ì–´ë–¤ ë°ì´í„°ì—ì„œ ì˜¨ ê±´ì§€ í‘œì‹œí•´ë‘ë©´ í•©ì³ì„œ ë´ë„ êµ¬ë¶„ì´ ëœë‹¤
    df["source"] = source
    return df


# ë‰´ìŠ¤ì™€ ë¸”ë¡œê·¸ëŠ” ê¸€ ìŠ¤íƒ€ì¼ì´ ë‹¬ë¼ì„œ ë‘˜ ë‹¤ ì¤€ë¹„í•´ë‘ê³  ì„ íƒí•´ì„œ ë³¼ ìˆ˜ ìˆê²Œ ë§Œë“¬.
df_news = load_data(NEWS_PATH, "news")
df_blog = load_data(BLOG_PATH, "blog")
df_all = pd.concat([df_news, df_blog], ignore_index=True)


st.sidebar.title("ë¶„ì„ ì˜µì…˜")

# ë‰´ìŠ¤ë§Œ ë³¼ì§€, ë¸”ë¡œê·¸ë§Œ ë³¼ì§€, ë‘˜ ë‹¤ í•©ì³ì„œ ë³¼ì§€ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•¨.
data_type = st.sidebar.selectbox("ë°ì´í„° íƒ€ì…", ["news", "blog", "all"], index=0)

if data_type == "news":
    df_raw = df_news.copy()
elif data_type == "blog":
    df_raw = df_blog.copy()
else:
    df_raw = df_all.copy()

if len(df_raw) == 0:
    st.error("ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. CSV íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# ë°ì´í„°ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë‚ ì§œ ì•ˆì—ì„œë§Œ ê³ ë¥´ê²Œ í•¨
min_d = df_raw["pubDate"].dt.date.min()
max_d = df_raw["pubDate"].dt.date.max()


def clamp_date_range(d1: date, d2: date, lo: date, hi: date) -> tuple[date, date]:
    # ë‚ ì§œ ì…ë ¥ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ë§ì¶°ì¤Œ
    if d1 < lo:
        d1 = lo
    if d1 > hi:
        d1 = hi
    if d2 < lo:
        d2 = lo
    if d2 > hi:
        d2 = hi
    if d1 > d2:
        d1, d2 = d2, d1
    return d1, d2


# ì²˜ìŒì—ëŠ” ì „ì²´ ë²”ìœ„ë¥¼ í•œ ë²ˆì— ë³¼ ìˆ˜ ìˆê²Œ ê¸°ë³¸ê°’ì„ ì¡ìŒ
default_start, default_end = clamp_date_range(min_d, max_d, min_d, max_d)

start_date, end_date = st.sidebar.date_input(
    "ë¶„ì„ ê¸°ê°„ ì„ íƒ",
    value=(default_start, default_end),
    min_value=min_d,
    max_value=max_d,
    key=f"date_range_{data_type}",
)

# ì œëª©ë§Œ ë³¼ì§€ ë³¸ë¬¸ê¹Œì§€ ê°™ì´ ë³¼ì§€ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ì„œ ì„ íƒí•˜ê²Œ í•¨
use_desc = st.sidebar.checkbox("title + description ì‚¬ìš©", value=True)

# ì›Œë“œí´ë¼ìš°ë“œëŠ” ë‹¨ì–´ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì½ê¸° ì–´ë ¤ì›Œì„œ ê°œìˆ˜ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆê²Œ í•¨
max_words = st.sidebar.slider("ì›Œë“œí´ë¼ìš°ë“œ ìµœëŒ€ ë‹¨ì–´ìˆ˜", 10, 200, 50)

# ê´€ê³„ë¥¼ ë„ˆë¬´ ì•½í•œ ê²ƒê¹Œì§€ ì—°ê²°í•˜ë©´ í™”ë©´ì´ ì§€ì €ë¶„í•´ì§€ë‹ˆ ìµœì†Œ ê¸°ì¤€ì„ ë‘ 
min_edge_weight = st.sidebar.slider("ë„¤íŠ¸ì›Œí¬ ë™ì‹œì¶œí˜„ ìµœì†Œ ë¹ˆë„", 2, 50, 10)

# ë„¤íŠ¸ì›Œí¬ ë…¸ë“œ ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë³´ê¸° ì–´ë ¤ì›Œì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ë‚¨ê¸°ë„ë¡ í•¨
top_n_nodes = st.sidebar.slider("ë„¤íŠ¸ì›Œí¬ì— í¬í•¨í•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜", 10, 100, 40)

# ê°ì„± ì ìˆ˜ëŠ” ê°„ë‹¨í•œ ë°©ì‹ê³¼ ì£¼ì œ ì¤‘ì‹¬ ë°©ì‹ì´ ë‹¤ë¥´ê²Œ ë³´ì¼ ìˆ˜ ìˆì–´ì„œ ì„ íƒí•˜ê²Œ í•¨
sentiment_mode = st.sidebar.radio("ê°ì„± ê¸°ì¤€ ì„ íƒ", ["ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜", "í‚¤ì›Œë“œ ê¸°ë°˜"], index=0)


df = df_raw.copy()

# ì‚¬ìš©ìê°€ ê³ ë¥¸ ê¸°ê°„ìœ¼ë¡œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì•¼ ì›í•˜ëŠ” êµ¬ê°„ë§Œ ë¶„ì„í•  ìˆ˜ ìˆë‹¤
df = df[(df["pubDate"].dt.date >= start_date) & (df["pubDate"].dt.date <= end_date)].copy()

# í…ìŠ¤íŠ¸ë¥¼ í•œ ì¹¸ì— ëª¨ì•„ë‘ë©´ ë’¤ì—ì„œ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤
if use_desc:
    df["text"] = (df["title"].fillna("") + " " + df["description"].fillna("")).astype(str)
else:
    df["text"] = df["title"].fillna("").astype(str)

st.write(f"í•„í„°ë§ í›„ ë¬¸ì„œ ìˆ˜: {len(df):,}ê°œ")
st.divider()


okt = Okt()


def clean_text_ko(text: str) -> str:
    # ì“¸ë°ì—†ëŠ” ê¸°í˜¸ë‚˜ íƒœê·¸ëŠ” ì œê±°
    text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r"[^ê°€-í£0-9A-Za-z\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def extract_nouns(text: str, stopwords: set) -> list[str]:
    # í•µì‹¬ ë‹¨ì–´ë§Œ ë½‘ì•„ì•¼ ì£¼ì œ íë¦„ì´ ë³´ì´ê¸°ì— ëª…ì‚¬ ìœ„ì£¼ë¡œ ê°€ì ¸ì˜´
    text = clean_text_ko(text)
    nouns = okt.nouns(text)
    nouns = [w for w in nouns if (len(w) > 1) and (w not in stopwords)]
    return nouns


# ê²€ìƒ‰ì–´ ìì²´ëŠ” í•µì‹¬ ìš”ì¸ì„ ê°€ë¦¬ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ ì œì™¸í•¨
base_stop = set(["ë°ëª¬", "í—Œí„°ìŠ¤", "ë„·í”Œë¦­ìŠ¤", "kíŒ", "ì¼€ì´íŒ", "ê´€ë ¨", "ì´ë²ˆ", "í†µí•´", "ê¸°ì"])
stopwords = set(base_stop)


all_nouns = []
for t in df["text"].tolist():
    all_nouns.append(extract_nouns(t, stopwords))


st.subheader("ì–¸ê¸‰ëŸ‰ íŠ¸ë Œë“œ")

if len(df) == 0:
    st.warning("í˜„ì¬ ì„¤ì •ì—ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì´ë‚˜ ë°ì´í„° íƒ€ì…ì„ ë°”ê¿”ë³´ì„¸ìš”.")
else:
    # ë‚ ì§œë³„ë¡œ ë¬¶ìœ¼ë©´ ê´€ì‹¬ì´ ì–¸ì œ ì§‘ì¤‘ëëŠ”ì§€ í•œëˆˆì— ë³¼ ìˆ˜ ìˆìŒ
    trend = df.copy()
    trend["date"] = trend["pubDate"].dt.date
    trend_cnt = trend.groupby("date").size().reset_index(name="count")

    chart = (
        alt.Chart(trend_cnt)
        .mark_line()
        .encode(x="date:T", y="count:Q", tooltip=["date:T", "count:Q"])
        .properties(height=300)
    )
    st.altair_chart(chart, use_container_width=True)

st.divider()


# ê¸€ì˜ ë¶„ìœ„ê¸°ë¥¼ ëŒ€ëµ ë³´ê¸° ìœ„í•œ ë‹¨ì–´ ëª©ë¡ì„ ë¯¸ë¦¬ ì •í•´ë´„
pos_words = set(["í¥í–‰", "í˜¸í‰", "ì¸ê¸°", "ì—´í’", "ê°ë™", "ì™„ì„±ë„", "ê¸°ëŒ€", "ëŒ€ë°•", "ì¶”ì²œ", "í™”ì œ"])
neg_words = set(["ë…¼ë€", "í˜¹í‰", "ë¶€ì§„", "ì‹¤ë§", "ë¹„íŒ", "ë¬¸ì œ", "ì•„ì‰½", "ë¶ˆë§Œ"])

# íŒ¬ë¤ í˜•ì„± ìš”ì¸ì„ ì„¤ëª…í•  ë•Œ ìì£¼ ë‚˜ì˜¤ëŠ” ì£¼ì œ ë‹¨ì–´ë¥¼ ë”°ë¡œ ë¬¶ì–´ë´„
theme_pos = set(["ìŒì•…", "ë…¸ë˜", "í¼í¬ë¨¼ìŠ¤", "ì•ˆë¬´", "ë³´ì»¬", "ìŠ¤í† ë¦¬", "ìºë¦­í„°", "ë¹„ì£¼ì–¼", "ì—°ì¶œ", "ì„¸ê³„ê´€"])
theme_neg = set(["í‘œì ˆ", "ë…¼ë€", "ë¶ˆí˜¸", "í˜¹í‰"])


def sentiment_score_rule(nouns: list[str]) -> int:
    # ê¸ì • ë‹¨ì–´ëŠ” ë”í•˜ê³  ë¶€ì • ë‹¨ì–´ëŠ” ë¹¼ì„œ ë¶„ìœ„ê¸°ë¥¼ ì ìˆ˜ë¡œ ë§Œë“¬
    score = 0
    for w in nouns:
        if w in pos_words:
            score += 1
        if w in neg_words:
            score -= 1
    return score


def sentiment_score_keyword(nouns: list[str]) -> int:
    # ì£¼ì œ ë‹¨ì–´ ì¤‘ì‹¬ìœ¼ë¡œ ì ìˆ˜ë¥¼ ë§Œë“¤ë©´ ë¬´ì—‡ ë•Œë¬¸ì— ë°˜ì‘ì´ ë‚˜ì™”ëŠ”ì§€ ë³´ê¸° ì‰¬ì›€
    score = 0
    for w in nouns:
        if w in theme_pos:
            score += 1
        if w in theme_neg:
            score -= 1
    return score


st.subheader("ê°ì„± ë¶„í¬")

if len(df) == 0:
    st.warning("í˜„ì¬ ì„¤ì •ì—ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì´ë‚˜ ë°ì´í„° íƒ€ì…ì„ ë°”ê¿”ë³´ì„¸ìš”.")
else:
    # ê°™ì€ ë°ì´í„°ë¼ë„ ì ìˆ˜ ê¸°ì¤€ì´ ë‹¬ë¼ì§€ë©´ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ì„œ ì„ íƒ ê°’ì„ ë°˜ì˜í•¨
    if sentiment_mode == "ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜":
        df["sentiment_score"] = [sentiment_score_rule(n) for n in all_nouns]
    else:
        df["sentiment_score"] = [sentiment_score_keyword(n) for n in all_nouns]

    fig, ax = plt.subplots(figsize=(8, 4))
    sns.histplot(df["sentiment_score"], bins=15, kde=True, ax=ax)
    ax.set_title("Sentiment Score Distribution")
    st.pyplot(fig)

st.divider()


st.subheader("í•µì‹¬ í‚¤ì›Œë“œ")

# ì „ì²´ ë‹¨ì–´ë¥¼ í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ì•„ì•¼ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ë¥¼ ì‰½ê²Œ ë½‘ì„ ìˆ˜ ìˆìŒ
flat = [w for doc in all_nouns for w in doc]
freq = Counter(flat)

if len(flat) == 0:
    st.warning("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì´ë‚˜ í…ìŠ¤íŠ¸ ë²”ìœ„ë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
else:
    # ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ë¶€í„° ë³´ì—¬ì£¼ë©´ ì‚¬ëŒë“¤ì´ ë­˜ ì¤‘ì‹¬ìœ¼ë¡œ ì–˜ê¸°í•˜ëŠ”ì§€ ë¹ ë¥´ê²Œ ì¡í˜
    top_df = pd.DataFrame(freq.most_common(20), columns=["keyword", "count"])
    fig_bar = px.bar(top_df, x="keyword", y="count", title="Top Keywords")
    st.plotly_chart(fig_bar, use_container_width=True)

st.subheader("ì›Œë“œí´ë¼ìš°ë“œ")

if len(flat) == 0:
    st.warning("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    # í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ í°íŠ¸ë¥¼ ë¨¼ì € ì¡ì•„ë‘ 
    try:
        han_font_path = font_manager.findfont("AppleGothic")
    except:
        han_font_path = None

    
    wc = WordCloud(
        font_path=han_font_path,
        max_words=max_words,
        stopwords=STOPWORDS,
        width=900,
        height=500,
        background_color="white",
    ).generate(" ".join(flat))

    fig_wc, ax_wc = plt.subplots(figsize=(12, 6))
    ax_wc.imshow(wc, interpolation="bilinear")
    ax_wc.axis("off")
    st.pyplot(fig_wc)

st.divider()


st.subheader("í‚¤ì›Œë“œ ê´€ê³„ë§")

if len(flat) == 0:
    st.warning("ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    # í•µì‹¬ í‚¤ì›Œë“œë§Œ ë‚¨ê¸°ë©´ ê´€ê³„ë§ì´ ë„ˆë¬´ ë³µì¡í•´ì§€ì§€ ì•ŠìŒ
    top_keywords = set([w for w, _ in freq.most_common(top_n_nodes)])

    # ê°™ì€ ê¸€ì—ì„œ ê°™ì´ ë‚˜ì˜¨ ë‹¨ì–´ëŠ” ê°™ì€ íë¦„ì—ì„œ ì–¸ê¸‰ëì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    edge_list = []
    for nouns in all_nouns:
        nouns = list(set(nouns) & top_keywords)
        if len(nouns) > 1:
            edge_list.extend(combinations(sorted(nouns), 2))

    edge_counts = Counter(edge_list)

    # ë„ˆë¬´ ë“œë¬¸ ì—°ê²°ì€ ìš°ì—°ì¼ ìˆ˜ ìˆì–´ì„œ ì¼ì • ê¸°ì¤€ ì´ìƒë§Œ ë‚¨ê¹€
    filtered_edges = {e: w for e, w in edge_counts.items() if w >= min_edge_weight}

    G = nx.Graph()
    G.add_weighted_edges_from([(u, v, w) for (u, v), w in filtered_edges.items()])

    st.write(f"ë…¸ë“œ ìˆ˜: {G.number_of_nodes():,}ê°œ")
    st.write(f"ì—£ì§€ ìˆ˜: {G.number_of_edges():,}ê°œ")

    if G.number_of_nodes() == 0:
        st.warning("ë„¤íŠ¸ì›Œí¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìµœì†Œ ë¹ˆë„ë¥¼ ë‚®ì¶”ê±°ë‚˜ ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")
    else:
        # ì—°ê²°ëœ ë‹¨ì–´ë¼ë¦¬ëŠ” ê°€ê¹Œì´ ë³´ì´ê²Œ ë°°ì¹˜í•˜ë©´ íë¦„ì´ ëˆˆì— ë“¤ì–´ì˜´
        pos = nx.spring_layout(G, k=0.35, iterations=80, seed=42)
        node_sizes = [G.degree(n) * 120 for n in G.nodes()]
        edge_widths = [G[u][v]["weight"] * 0.06 for u, v in G.edges()]

        fig_net = plt.figure(figsize=(12, 12))
        nx.draw_networkx(
            G,
            pos,
            with_labels=True,
            node_size=node_sizes,
            width=edge_widths,
            font_family=plt.rcParams["font.family"],
            font_size=10,
            node_color="skyblue",
            edge_color="gray",
            alpha=0.85,
        )
        plt.title("Keyword Co-occurrence Network")
        plt.axis("off")
        st.pyplot(fig_net)